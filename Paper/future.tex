\section{Future Work}
\label{sec:future}

While our approach to handling metadata writes is quite naive in it's
implementation and simply a proof of concept, there are more efficient
ways to handle writing metadata to SSD's such as the FD-Tree
structure~\cite{FD-Tree} and Flash B-tree's~\cite{FlashB-Tree}. Both
of these data structures lead to similar improvements in random write
throughput, lower energy consumption, and more efficient handling of
write-wearing. We plan on extending our current approach with this
work for more efficient metadata persistence mechanisms.

We can make the overhead costs and how often metadata should be
persisted variable values depending on how tolerant the system should
be or what the acceptable overhead is for maintaining consistency
between the cache and its metadata. This way it is more applicable to
more workloads which may fall under tighter data constraints.

Although our current implementation had issues with write back it's
possible that this would benefit from fault tolerance as well. In
cases where there is a system failure and you currently have dirty
blocks in the cache this could mean data loss but if the metadata
representing those blocks could be recovered then the dirty blocks
could be written back without loss of data.

We only evaluated our persistence mechanism on two SSD's namely an
Mtron and Vertex SSD. For future work we will run tests on more SSD's
as it seems that write performance of SSD's can vary widely between
manufacturers.

Our batching efficiency was pretty good already mostly due to how we
pack a lot of metadata into one block and write the whole block at
once. DM-Cache currently uses the kernel function
\texttt{hash\_long()} to calculate the hash values for the blocks to
place in its internal mapping and it's possible that we will benefit
more from matching if we use a uniform distribution that places
sequential physical blocks sequentially in the mapping. This will give
us more locality and help avoid even more writes since most accesses
are sequential and we can issue many of these updates in one write
operation. Another possibility is to use a function that depends on
how often the metadata gets updated this way ``hot'' metadata that is
updated more often is grouped together which allows us to get even
more benefit from batched metadata updates.
