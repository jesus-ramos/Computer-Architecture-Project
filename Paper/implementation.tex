\section{Design and Implementation}
\label{sec:implementation}

\subsection{Design Considerations}

The two main design considerations of our implementation are data
consistency and overhead. With data consistency we want to make sure
that we never have invalid data in the cache's metadata that could
cause corruption. We also want to make sure that we have an acceptable
overhead so as to not impact the performance of the cache.

\subsection{Implementation}

We leverage current mechanisms implemented in DM-Cache for metadata
management. The current implementation employs a persistence scheme
that allows you to write all the metadata for the cache upon
destruction of the cache. The problem with this implementation is that
in the event of a system failure we don't have the convenience of this
code being executed so all metadata is lost.

To overcome these limitations we came up with a new way to represent
metadata on the cache device as well as a persistence mechanism that
ensures data consistency.

\subsubsection{Metadata Representation}

The metadata representation in the current version of DM-Cache only
takes into account valid blocks or blocks that do not have pending
changes in the cache. We extend this by allowing it to also handle
blocks that have been marked as dirty and are pending to be written
back to the storage device. We do this by creating a small structure
which contains the sector number and the state of that block on the
disk and write this to the end of the disk along with a metadata
header for the whole cache. We can use this header to determine how
many blocks of metadata to read from the disk in order to restore
previous cache state.

Internally DM-Cache uses a hash table to represent the metadata for
the blocks. Our mechanism uses the same representation on disk. We use
the last sector of the specified cache device to store a metadata
header which has information about the cache itself such as size and
number of entries. We write our metadata sequentially starting after
this metadata header. A limit of the current implementation is that
the size of the cache itself has to be a perfect power of two due to
the way that the indexes in the hash table are calculated. This allows
us to safely use those sectors for simplicity of implementation.

To persist this information we bundle our metadata into blocks which
are the same size as the block size of the device and simply try and
fit as many as we can into one block. We do this because each metadata
entry that should be persisted is a lot smaller than the devices block
size and as such we can bundle many of them together. This way we
write the smallest possible amount of data for every metadata update
while avoiding having to issue a lot of writes for metadata that is
updated in a short period of time.

When the cache is being reconstructed we simply read this header off
the end of the disk to reinitialize the cache parameters and using
this data we also read the metadata and restore the mapping in the
cache to its previous state.

\subsubsection{Persistence Mechanism}

In DM-Cache all work that does I/O is managed through a central work
queue. Whenever a cached block is either: \textcolor{red}{methods that
  call write\_metadata()}, we mark the metadata block that needs to be
written to disk and place it on the work queue along with the I/O for
the actual data.

We write the metadata after the actual data commits. There is a small
chance that the data update can fail and hence we have invalid or
corrupt data in the cache but because the metadata is committed to
disk afterwards the metadata on disk does not reflect this. We believe
this chance to be very small and using a journaling style approach
where we invalidate the metadata for that entry, write the data, and
then validate it once we are sure the data is written has a higher
overhead. During our testing we didn't find any instances where there
was corrupt data when restoring the cache from metadata after a
simulated failure.

In the event of a failure you can choose load the metadata that was
present on the disk at the time of failure. During initialization we
read the metadata header stored at the end of the disk first and using
that information restore the cache to its previous state by reading
the remaining metadata and restoring the previous cache mappings.
